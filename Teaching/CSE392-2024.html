<html><head>
		<meta http-equiv="content-type" content="text/html; charset=windows-1252">
		<title>CSE 392: Matrix and Tensor Algorithms for Data, Spring 2024</title>
	</head>

<body>

<p></p>
<hr>
<p></p>

<h1> 
	University of Texas at Austin, Spring 2024
	<br>	<br>
CSE 392: Matrix and Tensor Algorithms for Data
</h1>

<p></p>
<hr>
<p></p>

<h3>Instructor</h3> 
<a href="https://shashankaubaru.github.io/"><b>Shashanka Ubaru</b> </a> 
<li>Email: shashanka.ubaru(at)austin.utexas.edu or (at)ibm.com</li>
<li>Office hours:  Mondays 1:30pm - 2:30pm.</li>
<li>Location: POB 3.134.</li> 

<p></p>


<b>Class time and Location:</b>
<li>Mondays and Wednesdays, 11:00am - 12:30pm, GDC 2.402.</li>

<p></p>
<hr>
<p></p>


<h3>Course description</h3>
With the advent of modern technology, internet, and social networks, machine learning and data scientists
today have to deal with large volumes of data of massive sizes. In this course, we study the
mathematical foundations of large-scale data processing, designing algorithms and learning to (theoretically)
analyze them. We explore randomized numerical linear algebra (sketching and sampling) and
tensor methods for processing and analyzing large-scale databases, graphs, data streams, and large multidimensional
data. We will also have presentations on linear algebra concepts of quantum computing.
<p></p>

<b>Prerequisites:</b> 
The minimum requirements for the course are basics concepts of probability, algorithms, and linear algebra.
Knowledge and experience with machine learning algorithms will be helpful. For the course,
we will rely most heavily on probability, linear and tensor algebra, but we will also learn some approximation
theory, high dimensional geometry, and quantum computing. The course will involve rigorous
theoretical analysis and some programming (practical implementation and applications).

<p></p>

<b>Programming language:</b> The programming languages for the course will be <i>Matlab</i> and/or <i>Python</i>.

<p></p>

<b>Syllabus:</b> <a href="Course_syllabus.pdf">PDF</a>

<p></p>

<p></p>
<hr>
<p></p>

<h3>Grading</h3> 
Grading is based on problem sets, scribing a lecture, and a presentation/project. There will be no exams.
The breakdown is as follows:

<ul>
	<li>
	<i>Scribing - 10%:</i> 1 - 2 lectures. LaTeX template for the notes can be found <a href='scribe_template.tex'>here</a>.
	</li>
  
	<li>
	<i>Assignments - 50%:</i> Four assignments including problem sets and programming exercises.
	</li>
  
	<li>
	<i>Class project - 40%:</i> Teams of two. There will be a final presentation of the projects during the last week of the semester.
	</li>
  
  </ul>

  Assignments are to be submitted through Canvas, and should be individual work. You are allowed to discuss the problems with your
classmates and to work collaboratively. The preferred format is to upload your work as a single PDF, preferably typewritten (using LaTeX,
Markdown, or some other mathematical formatting program).
In general, late assignments will not receive credit.

<!--
%-----------------------------------------------------------------------
-->

<p></p>
<hr>
<p></p>

<h3> Lectures</h3>


<table cellpadding="2" cellspacing="2" border="1" style="text-align: left;">
	<tbody>
	  <tr>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Dates</span><br>
		</td>
		<td style="vertical-align: top;width:630px"><span style="font-weight:bold;">Topics covered</span><br>
		</td>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Slides</span><br>
		</td>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Scribed notes</span><br>
		</td>
	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 1 (Jan 17, 2024)
		</td>
		<td style="vertical-align: top;">
		Lecture 1: 	Vector spaces, matrices, norms.
		<br>
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_1.pdf">Lecture 1</a>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 2 (Jan 22, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 2:	 Probability review, concentration of measure.
		<br>
		Lecture 3: Least squares regression, kernel methods.
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_2.pdf">Lecture 2</a> <br>
		<a href="Lecture_3.pdf">Lecture 3</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
   <tr>
		<td style="vertical-align: top;">
		Week 3 (Jan 29, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 4: Matrix factorizations I - SVD, QR.
		<br>
		Lecture 5: Matrix factorizations II - eigenvalue decomposition, PCA.
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_4.pdf">Lecture 4</a> <br>
		<a href="Lecture_5.pdf">Lecture 5</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
     <tr>
		<td style="vertical-align: top;">
		Week 4 (Feb 5, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 6: Approximate matrix product, sampling.
		<br>
		Lecture 7: Johnsonâ€“Lindenstrauss(JL) lemma, subspace embedding.
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_6.pdf">Lecture 6</a> <br>
		<a href="Lecture_7.pdf">Lecture 7</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>

		     <tr>
		<td style="vertical-align: top;">
		Week 5 (Feb 12, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 8: Sketching, types of sketching matrices.
		<br>
		Lecture 9: Sketch and solve - least squares regression.
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_8.pdf">Lecture 8</a> <br>
		<a href="Lecture_9.pdf">Lecture 9</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
				     <tr>
		<td style="vertical-align: top;">
		Week 6 (Feb 19, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 10: Sampling for least squares, preconditioned LS.
		<br>
		Lecture 11: Randomized SVD.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_10.pdf">Lecture 10</a> <br>
			<a href="Lecture_11.pdf">Lecture 11</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>

					     <tr>
		<td style="vertical-align: top;">
		Week 7 (Feb 26, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 12: Subspace iteration (power) method.
		<br>
		Lecture 13: Krylov subspace method.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_12.pdf">Lecture 12</a> <br>
			<a href="Lecture_13.pdf">Lecture 13</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
  				     <tr>
		<td style="vertical-align: top;">
		Week 8 (Mar 4, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 14: Stochastic trace estimation.
		<br>
		Lecture 15: Introduction to tensors, tensor-matrix product.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_14.pdf">Lecture 14</a>, 
			<a href="Spectral_sums.pdf">Spectral sums</a> <br>
			<a href="Lecture_15.pdf">Lecture 15</a>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
		 <tr>
		<td style="vertical-align: top;">
		Week 9 (Mar 11, 2024)
		</td>
					<td style="vertical-align: top;">
						<b> Spring Break </b> 
		</td> 
			 	<td style="vertical-align: top;">
		
		</td>
			 	<td style="vertical-align: top;">
		
		</td>
	  </tr>
  				     <tr>
		<td style="vertical-align: top;">
		Week 10 (Mar 18, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 16: Canonical Polyadic (CP) decomposition.
		<br>
		Lecture 17: Randomized CP - I.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_16.pdf">Lecture 16</a> <br>
			<a href="Lecture_17.pdf">Lecture 17</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
		 <tr>
		<td style="vertical-align: top;">
		Week 11 (Mar 25, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 18: Randomized CP - II.
		<br>
		Lecture 19: Tucker decomposition, HOSVD.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_18.pdf">Lecture 18</a> <br>
			<a href="Lecture_19.pdf">Lecture 19</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
				 <tr>
		<td style="vertical-align: top;">
		Week 12 (Apr 1, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 20: Randomized Tucker, TensorSketch.
		<br>
		Lecture 21: Tube-fiber product, t-product.
		</td>
		<td style="vertical-align: top;">
			<a href="Lecture_20.pdf">Lecture 20</a> <br>
			<a href="Lecture_21.pdf">Lecture 21</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
					 <tr>
		<td style="vertical-align: top;">
		Week 13 (Apr 8, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 22:  t-SVD, M-product.
		<br>
		Lecture 23: Randomized t-SVD, t-product applications.
		</td>
		<td style="vertical-align: top;">

		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
	</tbody>
  </table>



  <p></p>
  <hr>
  <p></p>
  
  <h3> Problem Sets</h3>
	<a href="HW1.pdf">Homework 1</a> <br>
	<a href="HW2.pdf">Homework 2</a> <br>
	<a href="HW3.pdf">Homework 3</a> <br>
	<a href="HW4.pdf">Homework 4</a> <br>

<p></p>
<hr>
<p></p>

<h3> Resources</h3>

<ul>
	<li> Dr. David Woodruff's monograph <a href="https://arxiv.org/abs/1411.4357">Sketching as a Tool for Numerical Linear Algebra</a>. 
</li>
<li> Dr. Tamara Kolda's review paper on <a href="https://www.kolda.net/publication/TensorReview.pdf">Tensor Decompositions and	Applications</a>. 
</li>
  <li> Dr. Yousef Saad's textbook <a href="http://www.cs.umn.edu/~saad/eig_book_2ndEd.pdf">Numerical Methods for Large
Eigenvalue Problems</a>. 
</li>
</ul>
<p></p>
<hr>
<p></p>





<script>if(window.parent==window){(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","//www.google-analytics.com/analytics.js","ga");ga("create","G-6FTL175Z2R","auto",{"siteSpeedSampleRate":100});ga("send","pageview");}</script>
</body></html>
