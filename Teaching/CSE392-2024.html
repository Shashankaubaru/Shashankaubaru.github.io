<html><head>
		<meta http-equiv="content-type" content="text/html; charset=windows-1252">
		<title>CSE 392: Matrix and Tensor Algorithms for Data, Spring 2024</title>
	</head>

<body>

<p></p>
<hr>
<p></p>

<h1> 
	University of Texas at Austin, Spring 2024
	<br>	<br>
CSE 392: Matrix and Tensor Algorithms for Data
</h1>

<p></p>
<hr>
<p></p>

<h3>Instructor</h3> 
<a href="https://shashankaubaru.github.io/"><b>Shashanka Ubaru</b> </a> 
<li>Email: shashanka.ubaru(at)austin.utexas.edu or (at)ibm.com</li>
<li>Office hours:  Mondays 1:30pm - 2:30pm.</li>
<li>Location: POB 3.134.</li> 

<p></p>


<b>Class time and Location:</b>
<li>Mondays and Wednesdays, 11:00am - 12:30pm, GDC 2.402.</li>

<p></p>
<hr>
<p></p>


<h3>Course description</h3>
With the advent of modern technology, internet, and social networks, machine learning and data scientists
today have to deal with large volumes of data of massive sizes. In this course, we study the
mathematical foundations of large-scale data processing, designing algorithms and learning to (theoretically)
analyze them. We explore randomized numerical linear algebra (sketching and sampling) and
tensor methods for processing and analyzing large-scale databases, graphs, data streams, and large multidimensional
data. We will also have presentations on linear algebra concepts of quantum computing.
<p></p>

<b>Prerequisites:</b> 
The minimum requirements for the course are basics concepts of probability, algorithms, and linear algebra.
Knowledge and experience with machine learning algorithms will be helpful. For the course,
we will rely most heavily on probability, linear and tensor algebra, but we will also learn some approximation
theory, high dimensional geometry, and quantum computing. The course will involve rigorous
theoretical analysis and some programming (practical implementation and applications).

<p></p>

<b>Programming language:</b> The programming languages for the course will be <i>Matlab</i> and/or <i>Python</i>.

<p></p>

<b>Syllabus:</b> <a href="Course_syllabus.pdf">PDF</a>

<p></p>

<p></p>
<hr>
<p></p>

<h3>Grading</h3> 
Grading is based on problem sets, scribing a lecture, and a presentation/project. There will be no exams.
The breakdown is as follows:

<ul>
	<li>
	<i>Scribing - 10%:</i> 1 - 2 lectures. LaTeX template for the notes can be found <a href='scribe_template.tex'>here</a>.
	</li>
  
	<li>
	<i>Assignments - 50%:</i> 4 to 5 assignments each contributing an equal amount to the grade. Assignments will
	include problem sets and programming exercises.
	</li>
  
	<li>
	<i>Class project - 40%:</i> Teams of two. THere will be a final presentation of the projects during the last week of the semester.
	</li>
  
  </ul>

  Assignments are to be submitted through Canvas, and should be individual work. You are allowed to discuss the problems with your
classmates and to work collaboratively. The preferred format is to upload your work as a single PDF, preferably typewritten (using LaTeX,
Markdown, or some other mathematical formatting program).
In general, late assignments will not receive credit.

<!--
%-----------------------------------------------------------------------
-->

<p></p>
<hr>
<p></p>

<h3> Lectures</h3>


<table cellpadding="2" cellspacing="2" border="1" style="text-align: left;">
	<tbody>
	  <tr>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Dates</span><br>
		</td>
		<td style="vertical-align: top;width:630px"><span style="font-weight:bold;">Topics covered</span><br>
		</td>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Slides</span><br>
		</td>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Scribed notes</span><br>
		</td>
	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 1 (Jan 17, 2024)
		</td>
		<td style="vertical-align: top;">
		Lecture 1: 	Vector spaces, matrices, norms.
		<br>
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_1.pdf">Lecture 1</a>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 2 (Jan 22, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 2:	 Probability review, concentration of measure.
		<br>
		Lecture 3: Least squares regression, kernel methods.
		</td>
		<td style="vertical-align: top;">
		<a href="Lecture_2.pdf">Lecture 2</a> <br>
		<a href="Lecture_3.pdf">Lecture 3</a> <br>
		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
   <tr>
		<td style="vertical-align: top;">
		Week 3 (Jan 29, 2024)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 4 Matrix factorizations I - SVD, QR.
		<br>
		Lecture 5: Matrix factorizations II - eigenvalue decomposition, PCA.
		</td>
		<td style="vertical-align: top;">

		</td>
		<td style="vertical-align: top;">
		
		</td>
	  </tr>
  
  
	</tbody>
  </table>



  <p></p>
  <hr>
  <p></p>
  
  <h3> Problem Sets</h3>


<p></p>
<hr>
<p></p>

<h3> Resources</h3>

<ul>
	<li> Dr. David Woodruff's monograph <a href="https://arxiv.org/abs/1411.4357">Sketching as a Tool for Numerical Linear Algebra</a>. 
</li>
<li> Dr. Tamara Kolda's review paper on <a href="https://www.kolda.net/publication/TensorReview.pdf">Tensor Decompositions and	Applications</a>. 
</li>
  <li> Dr. Yousef Saad's textbook <a href="http://www.cs.umn.edu/~saad/eig_book_2ndEd.pdf">Numerical Methods for Large
Eigenvalue Problems</a>. 
</li>
</ul>
<p></p>
<hr>
<p></p>





<script>if(window.parent==window){(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","//www.google-analytics.com/analytics.js","ga");ga("create","G-6FTL175Z2R","auto",{"siteSpeedSampleRate":100});ga("send","pageview");}</script>
</body></html>
