<html><head>
		<meta http-equiv="content-type" content="text/html; charset=windows-1252">
		<title>CSE 392: Matrix and Tensor Algorithms for Data, Spring 2025</title>
	</head>

<body>

<p></p>
<hr>
<p></p>

<h1> 
	University of Texas at Austin, Spring 2025
	<br>	<br>
CSE 392/CS 395T/M 397C: Matrix and Tensor Algorithms for Data
</h1>

<p></p>
<hr>
<p></p>

<h3>Instructor</h3> 
<a href="https://shashankaubaru.github.io/"><b>Shashanka Ubaru</b> </a> 
<li>Email: shashanka.ubaru(at)austin.utexas.edu</li>
<li>Office hours:  Wednesdays 12:30pm - 1:30pm.</li>
<li>Location: POB: 3.134.</li> 

<p></p>


<b>Class time and Location:</b>
<li>Mondays and Wednesdays, 11:00am - 12:30pm, GDC 2.402.</li>

<p></p>
<hr>
<p></p>


<h3>Course description</h3>
Advances in modern technologies have resulted in huge volumes of data being generated in several scientific, industrial, and social domains. With ever increasing size of data comes the necessity to develop fast and scalable machine learning and data algorithms to process and analyze them. In this course, we study the mathematical foundations of large-scale data processing, with focus on  designing algorithms and learning to (theoretically) analyze them. We explore randomized numerical linear algebra (sketching and sampling) and tensor methods for processing and analyzing large-scale multidimensional data, graphs, and data-streams. We will also have presentations on the linear algebra concepts of quantum computing.
<p></p>

<b>Prerequisites:</b> 
The minimum requirements for the course are basics concepts of probability, algorithms, and linear algebra. Knowledge and experience with machine learning algorithms will be helpful. For the course, we will rely most heavily on probability, linear and tensor algebra, but we will also learn few concepts related to approximation theory, high dimensional geometry, and  quantum computing. The course will involve rigorous theoretical analyses and some programming (practical implementation and applications). 

<p></p>

<b>Programming language:</b> The programming languages for the course will be <i>Matlab</i> and/or <i>Python</i>.

<p></p>

<b>Syllabus:</b> <a href="2025/Course_syllabus.pdf">PDF</a>

<p></p>

<p></p>
<hr>
<p></p>

<h3>Grading</h3> 
Grading is based on problem sets,  project/presentation, and class participation. There will be no exams.  The breakdown is as follows:

<ul>
	
  
	<li>
	<i>Assignments - 50%:</i> Four assignments including problem sets and programming exercises.
	</li>
  
	<li>
	<i>Class project - 40%:</i>  There will be a project proposal submission (before Spring break), and a final presentation of the projects during the last week of the semester, along with a final report submission.
	</li>
	
	<li>
	<i>Participation- 10%:</i> Participation in the class</a>.
	</li>
  
  </ul>

 Assignments are to be submitted through Canvas, and should be individual work. You are allowed to discuss the problems with your
classmates and to work collaboratively. The preferred format is to upload your work as a single PDF, preferably typewritten (using LaTeX,
Markdown, or some other mathematical formatting program).
In general, late assignments will not receive credit.

<!--
%-----------------------------------------------------------------------
-->

<p></p>
<hr>
<p></p>

<h3> Lectures</h3>


<table cellpadding="2" cellspacing="2" border="1" style="text-align: left;">
	<tbody>
	  <tr>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Dates</span><br>
		</td>
		<td style="vertical-align: top;width:630px"><span style="font-weight:bold;">Topics covered</span><br>
		</td>
		<td style="vertical-align: top;width:220px"><span style="font-weight:bold;">Slides</span><br>
		</td>

	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 1 (Jan 13, 2025)
		</td>
		<td style="vertical-align: top;">
		Lecture 1: Vector spaces, matrices, and norms.
		<br>
		Lecture 2: Probability review, concentration of measure.
		</td>
		<td style="vertical-align: top;">
		<a href="2025/Lecture_1.pdf">Lecture 1</a>
			<br>
			<a href="2025/Lecture_2.pdf">Lecture 2</a>
		</td>
	  </tr>
  
	  <tr>
		<td style="vertical-align: top;">
		Week 2 (Jan 20, 2025)
		</td>
		<td style="vertical-align: top;">
		<b>Martin Luther King, Jr. Day</b>
		<br>
		Lecture 3: Least squares regression, kernel methods.
		</td>
		<td style="vertical-align: top;">
			<br>
			<a href="2025/Lecture_3.pdf">Lecture 3</a>
		</td>
	  </tr>
  	  <tr>
		<td style="vertical-align: top;">
		Week 3 (Jan 27, 2025)
		</td>
		<td style="vertical-align: top;">
		Lecture 4: Matrix factorizations I - SVD, QR.
			<br>
		Lecture 5: Matrix factorizations II - eigenvalue decomposition, PCA. 
		</td>
		<td style="vertical-align: top;">
		<a href="2025/Lecture_4.pdf">Lecture 4</a>
			<br>
		<a href="2025/Lecture_5.pdf">Lecture 5</a>
		</td>
	  </tr>

	<tr>
		<td style="vertical-align: top;">
		Week 4 (Feb 3, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 6: Approximate matrix product, sampling.
		<br>
		Lecture 7: Johnsonâ€“Lindenstrauss(JL) lemma, subspace embedding.
		</td>
		<td style="vertical-align: top;">
			<a href="2025/Lecture_6.pdf">Lecture 6</a>
			<br>
			<a href="2025/Lecture_7.pdf">Lecture 7</a>
		</td>
	  </tr>

	<tr>
		<td style="vertical-align: top;">
		Week 5 (Feb 10, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 8: Sketching, types of sketching matrices.
		<br>
		Lecture 9: Sketch and solve - least squares regression.
		</td>
		<td style="vertical-align: top;">
			<a href="2025/Lecture_8.pdf">Lecture 8</a>
			<br>
			<a href="2025/Lecture_9.pdf">Lecture 9</a>
		</td>
	  </tr>
		<tr>
		<td style="vertical-align: top;">
		Week 6 (Feb 17, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 10: Sampling for least squares, preconditioned LS.
		<br>
		Lecture 11: Randomized SVD.
		</td>
		<td style="vertical-align: top;">
			<a href="2025/Lecture_10.pdf">Lecture 10</a>
			<br>
			<a href="2025/Lecture_11.pdf">Lecture 11</a>
		</td>
	  </tr>
		<tr>
		<td style="vertical-align: top;">
		Week 7 (Feb 24, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 12: Subspace iteration (power) method.
		<br>
		Lecture 13: Krylov subspace method.
		</td>
		<td style="vertical-align: top;">
			 <a href="2025/Lecture_12.pdf">Lecture 12</a>
			<br>
			<a href="2025/Lecture_13.pdf">Lecture 13</a>
		</td>
	  </tr>

		<tr>
		<td style="vertical-align: top;">
		Week 8 (Mar 3, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 14: Stochastic trace estimation.
		<br>
		Lecture 15: Introduction to tensors, tensor-matrix product.
		</td>
		<td style="vertical-align: top;">
			<a href="2025/Lecture_14.pdf">Lecture 14</a>, 
			<a href="2025/Spectral_sums.pdf">Spectral sums</a> <br>
			<a href="2025/Lecture_15.pdf">Lecture 15</a>
		</td>
	  </tr>
		     <tr>
		<td style="vertical-align: top;">
		Week 9 (Mar 10, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 16: Canonical Polyadic (CP) decomposition.
		<br>
		Lecture 17: Randomized CP - I.
		</td>
		<td style="vertical-align: top;">
			 <a href="2025/Lecture_16.pdf">Lecture 16</a>
			<br>
			<a href="2025/Lecture_17.pdf">Lecture 17</a>
		</td>
	  </tr>
		  <tr>
		<td style="vertical-align: top;">
		Week 10 (Mar 17, 2025)
		</td>
		<td style="vertical-align: top;">
		<b>Spring Break</b>
		<br>
		</td>
		<td style="vertical-align: top;">
		</td>
	  </tr>
		<tr>
		<td style="vertical-align: top;">
		Week 11 (Mar 24, 2025)
		<br>
		</td>
		<td style="vertical-align: top;">
		Lecture 18: Randomized CP - II.
		<br>
		Lecture 19: Tucker decomposition, HOSVD.
		</td>
		<td style="vertical-align: top;">
		</td>
	  </tr>
	</tbody>
  </table>

  <p></p>
  <hr>
  <p></p>
  
  <h3> Problem Sets</h3>
<a href="2025/HW1.pdf">Homework 1</a> <br>
<a href="2025/HW2.pdf">Homework 2</a> <br>
<a href="2025/HW3.pdf">Homework 3</a> <br>
<p></p>
<hr>
<p></p>

<h3> Resources</h3>

<ul>
	<li> Dr. David Woodruff's monograph <a href="https://arxiv.org/abs/1411.4357">Sketching as a Tool for Numerical Linear Algebra</a>. 
</li>
<li> Dr. Tamara Kolda's review paper on <a href="https://www.kolda.net/publication/TensorReview.pdf">Tensor Decompositions and	Applications</a>. 
</li>
  <li> Dr. Yousef Saad's textbook <a href="http://www.cs.umn.edu/~saad/eig_book_2ndEd.pdf">Numerical Methods for Large
Eigenvalue Problems</a>. </li>
	  <li> A draft of the upcoming book of Dr. Tamara Kolda's <a href="https://www.mathsci.ai/post/tensor-textbook/">Tensor Decompositions for Data Science</a>. 	
</li>
  <li> Dr. Tamara Kolda's book on <a href="https://www.mathsci.ai/post/latex-graphics-book/">Unlocking Latex Graphics</a>. 	
</li>
</ul>
<p></p>
<hr>
<p></p>




<script>if(window.parent==window){(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,"script","//www.google-analytics.com/analytics.js","ga");ga("create","G-6FTL175Z2R","auto",{"siteSpeedSampleRate":100});ga("send","pageview");}</script>
</body></html>
