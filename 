<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Shashanka Ubaru</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>

	<body id="top">

		<!-- Header -->
			<header id="header">
				<a href="#" class="image"><img src="images/new.png"  width="200" height="210" /></a>
				<h1><strong>Shashanka Ubaru</strong></h1>
                                <p>
				Ph.D Student<br>
				University of Minnesota-Twin Cities</p>
  		<!-- Nav -->
			<nav id="nav">
					<ul>
						<li><a href="#one" class="active">Home</a></li>
						<li><a href="#two">About me</a></li>
						<li><a href="#three">Education</a></li>
						<li><a href="#four">Publications</a></li>
					</ul>
				</nav>
			</header>

		<!-- Main -->
			<div id="main">

				<!-- One -->
		<section id="one">
						<div class="row">
		<div class="col c8">
			<h1><a href="index.html"><b>Shashanka Ubaru</b></a></h1>
			<p class="slogan">
			<a href="http://www.cs.umn.edu/" target="_blank">Department of Computer Science and Engineering</a>,<br>
			<a href="http://www.umn.edu/" target="_blank">University of Minnesota-Twin Cities</a>
			</p>
		</div>
	
		<div class="col c8">
   			<ul class="labeled-icons">
	 		<li>
			<h3 class="icon fa-home"><span class="label">Address</span></h3>
			Kenneth H. Keller Hall<br>
			200 Union Street SE<br>
			Minneapolis, MN, USA 55455<br>
			Office: 5-250
			</li>
			<li>
			<h3 class="icon fa-envelope-o"><span class="label">Email</span></h3>
			<p><u>ubaru001(at)umn.edu</u></p> 
			</li>
			</ul>
		</div>
	</div>

	</section>

				<!-- Two -->
					<section id="two">
						<h2>About Me</h2>
						<p>
I am a Ph.D student in the <a href="http://www.cs.umn.edu/" target="_blank">Department of Computer Science and Engineering</a>, at <a href="http://www.umn.edu/" target="_blank">University of Minnesota</a> (UMN).
I am fortunate to have Prof. <a href="http://www-users.cs.umn.edu/~saad/" target="_blank">Yousef Saad</a> as my Ph.D advisor. I also work with Prof. <a href="https://people.cs.umass.edu/~arya/"target="_blank">Arya Mazumdar</a>, who co-advised my master's thesis. 
I am also a member of <a href="http://bouchardlab.lblsci.wpengine.com/" target="_blank">Bouchard Lab</a> at <a href="https://www.lbl.gov/" target="_blank">Lawrence Berkeley National Labs</a>. My research interests are in <i>Numerical Linear Algebra, Machine Learning</i> and <i>Signal Processing</i>. In particular, I am interested in the use of computational linear algebra tools for solving problems related to machine learning, data analysis and signal processing. I am also interested in the applications of error correcting codes in machine learning, and in material informatics.
					</p>

<a href='Ubaru_CV.pdf' role='button' class="button special small">Curriculum Vitae (pdf) </a>

										</section>

				<!-- Three -->
					<section id="three">
						
						<h2>Education</h2>
					<ul>
						<li><b><a href="http://www.umn.edu"target="_blank">University of Minnesota</a>, Minneapolis, MN, USA</b> 

 					<ul style="list-style-type:square">
					<li>
  					<p ><i>Ph.D. student in <a href="https://www.cs.umn.edu"target="_blank">Computer Science</a></i>
  					<span style="float:right;">Sept. 2014 - present</span><br />
				 	Advisor: <a href="http://www-users.cs.umn.edu/~saad"target="_blank">Yousef Saad</a>  </p>
					</li>

					<li><p style="text-align:left;"><i>M.S. in <a href="https://www.cs.umn.edu"target="_blank">Computer Science</a></i>
  					<span style="float:right;">Sept. 2013 - Oct. 2015</span><br />
					Project: <i>Low rank approximation using error correcting codes</i> <br />
				 	Advisor: <a href="http://www-users.cs.umn.edu/~saad"target="_blank">Yousef Saad</a>  </p>
					</li>

        				<li><p style="text-align:left;"><i>M.S. in <a href="http://www.ece.umn.edu"target="_blank">Electrical Engineering</a></i>
  					<span style="float:right;">Sept. 2012 - Nov. 2014</span><br />
					Thesis: <i>Randomized techniques for matrix decomposition and estimating the approximate matrix ranks</i><br />
				 	Advisors: <a href="http://www-users.cs.umn.edu/~saad"target="_blank">Yousef Saad</a>  
					and <a href="https://people.cs.umass.edu/~arya/"target="_blank">Arya Mazumdar</a></p>
					</li>
					</ul>
				</li>
				<li><b><a href={http://www.mrsit.edu"target="_blank">M.S. Ramaiah Institute of Technology</a>, Bangalore, India</b>
					<ul style="list-style-type:square">
					<li>
  					<p style="text-align:left;"><i>B.Eng. in <a href="http://www.msrit.edu/electronics-communication-engineering"target="_blank">Electronics and Communication</a></i>
  					<span style="float:right;">Sept. 2008 - May 2012</span><br />
				Final Project: <i>RADARS: Determining Doppler, Ranging and Imaging</i></p>
					</li>
				</li>
			</section>
				<!-- Four -->			
					<section id="four">
						<header>
						<h2>Publications</h2>
						</header>
<h3> In submission </h3>

<ol>

					<li> C. Musco, P. Netrapalli, A. Sifford, <b>S. Ubaru</b> and David P. Woodruff,
					"Spectrum Approximation Beyond Fast Matrix Multiplication: Algorithms and Hardness", 
					  2017. <br />
					<	[<a href="http://arxiv.org/abs/1704.04163" target="_blank"><i class="fa fa-external-link"></i> arXiv</a>]
					</li>

										<li> <b>S. Ubaru</b>, Jie Chen, and Y. Saad,
					"Fast estimation of <i>tr(f(A))</i> via Stochastic Lanczos Quadrature", 
					Revised  2017.
					[<a href="http://www-users.cs.umn.edu/~saad/PDF/ys-2016-04.pdf" target="_blank"><i class="fa fa-external-link"></i> PDF</a>]
					</li>

					<li> <b>S. Ubaru</b>, A. Mazumdar, and Y. Saad,
					"Low rank approximation and decomposition of large matrices using error correcting codes", 
					Submitted  2015 and Revised 2016.
					[<a href="http://arxiv.org/abs/1512.09156" target="_blank"><i class="fa fa-external-link"></i> arXiv</a>]
					</li>
					</ol>


<h3>In publication</h3>
        		
				<ol reversed>
<li> <b> Multilabel Classification with Group Testing and Codes </b>, </br>
<b><i>S. Ubaru</i></b> and <i> A. Mazumdar</i> </br>
International Conference on Machine Learning (ICML), 2017 (to appear)
</li>
					<li> <b>Formation enthalpies for transition metal alloys using machine learning, 
				<br/> <b><i>S. Ubaru</b>, Agnieszka Miedlar, Y. Saad</i> and <i>James R. Chelikowsky</i>,<br/>
Physcal Review B,	<i>Accepted</i>  2017. 
					[<a href="http://www-users.cs.umn.edu/~saad/PDF/ys-2016-05.pdf" target="_blank"><i class="fa fa-external-link"></i> PDF</a>]
					</li>

<li> 
					<b>Fast estimation of approximate matrix ranks using spectral densities</b><br />
					<b><i>S. Ubaru</b>, Yousef Saad,</i> and <i>Abd-Krim Seghouane</i><br />
					Neural Computation,29(5):1317-1351, 2017 .<br />
					[<span>
<a style="cursor:pointer;" href="#_6"title="Show Abstract"><i class="fa fa-bars"></i> Abstract</a>, 
<a href="Papers/rank_DOS_ARXIV.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>,
<a href="http://arxiv.org/abs/1608.05754" target="_blank"><i class="fa fa-external-link"></i> arXiv</a>
</span>]
					</li>
<div class="collapse">
  <div id="_6">Many machine learning and data related applications require the knowledge of approximate
ranks of large data matrices at hand. This paper presents two computationally inexpensive techniques to estimate the approximate ranks of such matrices. These techniques exploit approximate
spectral densities, popular in physics, which are probability density distributions that measure the
likelihood of finding eigenvalues of the matrix at a given point on the real line. Integrating the spectral density over an interval gives the eigenvalue count of the matrix in that interval. Therefore the
rank can be approximated by integrating the spectral density over a carefully selected interval. Two
different approaches are discussed to estimate the approximate rank, one based on Chebyshev polynomials and the other based on the Lanczos algorithm. In order to obtain the appropriate interval, it
is necessary to locate a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that contribute to the matrix rank. A method for locating this gap and selecting the interval
of integration is proposed based on the plot of the spectral density. Numerical experiments illustrate
the performance of these techniques on matrices from typical applications.
</div>
</div>
					</li>
					<li> 
					<b>Improving the Incoherence of a Learned Dictionary via Rank Shrinkage</b><br />
					<b><i>S. Ubaru</b>, Abd-Krim Seghouane,</i> and <i>Yousef Saad</i><br />
					Neural Computation, 29(1):263-285, 2017<br />
					[<span>
<a style="cursor:pointer;" href="#_5"title="Show Abstract"><i class="fa fa-bars"></i> Abstract</a>, 
<a href="Papers/DL_rank_shrinkage.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>
</span>]
					</li>
<div class="collapse">
  <div id="_5">This letter considers the problem of dictionary learning for sparse signal representation whose atoms have
low mutual coherence. To learn such dictionaries, at each step we first updated the dictionary using the Method
of Optimal Directions (MOD), and then apply a dictionary rank shrinkage step to decrease its mutual coherence.
In the rank shrinkage step, we first compute a rank one decomposition of the column normalized least squares
estimate of the dictionary obtained from the MOD step. We then shrink the rank of this learned dictionary by
transforming the problem of reducing the rank to a nonnegative garrotte estimation problem, and solving it using a
path-wise coordinate descent approach. We establish theoretical results which show that the rank shrinkage step
included will reduce the coherence of the dictionary, which is further validated by experimental results. Numerical
experiments illustrating the performance of the proposed algorithm in comparison to various other well known
dictionary learning algorithms are also presented.
</div>
</div>
					</li>
					<li> 
					<b>Fast methods for estimating the Numerical rank of large matrices</b><br />
					<b><i>S. Ubaru</b></i> and <i>Yousef Saad</i><br />
					In Proceedings of 33rd International Conference on Machine Learning (ICML), pp- 468-477, 2016.<br />
					   [<span>
<a style="cursor:pointer;" href="#_4"title="Show Abstract"><i class="fa fa-bars"></i> Abstract</a>
</span>,
					<a href="Papers/rank_estimation_final.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>,
					<a href="Papers/RE_supplementary.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> Supp</a>,
					<a href="http://jmlr.org/proceedings/papers/v48/" target="_blank"><i class="fa fa-external-link"></i> Link</a>,<span>
					<a style="cursor:pointer;" href="#popup1" title="Show BibTeX entry"><i class="fa fa-toggle-off"></i> BibTeX</a>					</span>, <a href="codes/rank_estimation.zip" target="_blank"><i class="fa fa-download"></i> Codes</a>]
					<br />
					</li>
					<div class="collapse">
  					<div id="_4">We present two computationally inexpensive techniques for estimating the numerical rank of a matrix, combining powerful tools from computational linear algebra. These techniques exploit three key ingredients. The first is to approximate the projector on the non-null invariant subspace of the matrix by using a polynomial filter. Two types of filters are discussed, one based on Hermite interpolation and the other based on Chebyshev expansions. The second ingredient employs stochastic trace estimators to compute the rank of this wanted eigen-projector, which yields the desired rank of the matrix. In order to obtain a good filter, it is necessary to detect a gap between the eigenvalues that correspond to noise and the relevant eigenvalues that correspond to the non-null invariant subspace. The third ingredient of the proposed approaches exploits the idea of spectral density, popular in physics, and the Lanczos spectroscopic method to locate this gap.
					</div>
					</div>
					<div id="popup1" class="overlay">
						<div class="popup">
							<a class="close" href="#">&times;</a>
						<div class="content">
						@inproceedings{ubaru2016fast, <br />
  						title={Fast methods for estimating the Numerical rank of large matrices},<br />
  						author={Ubaru, Shashanka  and Saad, Yousef},<br />
  						booktitle={Proceedings of the 33rd International Conference on Machine Learning (ICML-16)},<br />
 						pages={468-477},<br />
  						year={2016}}
						</div>
						</div>
					</div>
					<li> 
					<b>Group testing schemes from low-weight codewords of BCH codes</b><br />
					<b><i>S. Ubaru</b>, Arya Mazumdar,</i> and <i>Alexander Barg</i><br />
					IEEE International Symposium on Information Theory (ISIT), 2016.<br />		
					   [<span>
<a style="cursor:pointer;" href="#_3"title="Show Abstract"><i class="fa fa-bars"></i> Abstract</a>
</span>,
					<a href="Papers/GT_expt_ISIT_final.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>, <span>
					<a style="cursor:pointer;" href="#popup1" title="Show BibTeX entry"><i class="fa fa-toggle-off"></i>BibTeX</a>					</span>]
					<br />
					</li>
					<div class="collapse">
  					<div id="_3">Despite a large volume of research in group testing,
explicit small-size group testing schemes are still difficult to
construct, and the parameters of known combinatorial schemes
are limited by the constraints of the problem. Relaxing the worst-
case identification requirements to probabilistic localization of
defectives enables one to expand the range of parameters, and
yet the small-size practical constructions are sparse.</br>
Motivated by this question, we perform an experimental
study of almost disjunct matrices constructed from low-weight
codewords of binary BCH codes, and evaluate their performance
in nonadaptive group testing. We observe that identification
of defectives is much more stable in these schemes compared
to the schemes constructed from random binary matrices. We
derive an estimate of the error probability of identification in
the constructed schemes which provides a partial explanation of
their performance.
					</div>
					</div>
					<div id="popup1" class="overlay">
						<div class="popup">
							<a class="close" href="#">&times;</a>
						<div class="content">
						@inproceedings{ubaru2016group,<br />
  title={Group testing schemes from low-weight codewords of BCH codes},<br />
  author={Ubaru, Shashanka and Mazumdar, Arya and Barg, Alexander},<br />
   booktitle={IEEE International Symposium on Information Theory (ISIT)},<br />
  year={2016}<br />
}
						</div>
						</div>
					</div>


					<li> 
					<b>Low rank approximation using error correcting coding matrices</b><br />
					<b><i>S. Ubaru</b>, Arya Mazumdar,</i> and <i>Yousef Saad</i><br />
					In Proceedings of 32nd International Conference on Machine Learning (ICML), pp 702–710, 						2015.<br />
					[<span>
<a style="cursor:pointer;" href="#_2"title="Show Abstract"><i class="fa fa-navicon"></i> Abstract</a>
</span>,
					<a href="Papers/Low_Rank_ICML_Revised.pdf" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>, <a href="http://videolectures.net/icml2015_ubaru_coding_matrices/" target="_blank"> talk</a>, <a href="Papers/ICML_slides.pdf" target="_blank"> slides</a>,<span>
					<a style="cursor:pointer;" href="#popup2" title="Show BibTeX entry"><i class="fa fa-toggle-off"></i>BibTeX</a></span>]
					<div class="collapse">
  <div id="_2">Low-rank matrix approximation is an integral component of tools such as principal component
analysis (PCA), as well as is an important instrument used in applications like web search, text
mining and computer vision, e.g., face recognition. Recently, randomized algorithms were proposed to effectively construct low rank approximations of large matrices. </br>
In this paper, we show how matrices from error correcting codes can beused to find such low rank approximations.
The benefits of using these code matrices are the following: (i) They are easy to generate and 
they reduce randomness significantly. (ii) Code matrices have low coherence and have a better
chance of preserving the geometry of an entire subspace of vectors; (iii) Unlike Fourier transforms 
or Hadamard matrices, which require sampling O(k log k) columns for a rank-k approximation, 
the log factor is not necessary in the case of code matrices. (iv) Under certain conditions, the approximation 
errors can be better and the singular values obtained can be more accurate, than those obtained using Gaussian 
random matrices and other structured random matrices.
</div>
</div>

			                <div id="popup2" class="overlay">
						<div class="popup">
							<a class="close" href="#">&times;</a>
						<div class="content">
						@inproceedings{ubaru2015low, <br />
  						title={Low rank approximation using error correcting coding matrices},<br />
  						author={Ubaru, Shashanka and Mazumdar, Arya and Saad, Yousef},<br />
  						booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML-15)},<br />
 						pages={702--710},<br />
  						year={2015}}
						</div>
						</div>
					</div>

					<i>Published <a href="http://jmlr.org/proceedings/papers/v37/ubaru15.html">version</a> contains some minor 							errors that have since been corrected in the above revised version.</i> 
					</li>
					


					<li> 
					<b>Displaying gray scales by cross pairing select and data voltages in multi-line addressed LCD</b> <br />
					<b><i>S. Ubaru</i></b> and <i>Temkar N. Ruckmongathan</i><br />
					IEEE, Journal of Display Technology, vol 8, no. 11, pp 669–677, November, 2012.<br />
					[<span>
<a style="cursor:pointer;" href="#_1"title="Show Abstract"><i class="fa fa-bars"></i> Abstract</a>
</span>,
					<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6317119" target="_blank"><i class="fa fa-external-link"></i> Journal link</a>,
<span>
<a style="cursor:pointer;" href="#popup3" title="Show BibTeX entry"><i class="fa fa-toggle-off"></i> BibTeX</a>					</span>]
					</li>
<div class="collapse">
  <div id="_1">The idea of cross pairing of select and data voltages
in line-by-line addressing is extended for multi-line addressing; to
display a large number of gray scales with a small number of time
intervals as compared to successive approximation technique. The
method is illustrated by selecting two rows at a time with wave-
forms based on Hadamard matrix.</div>
</div>

					<div id="popup3" class="overlay">
						<div class="popup">
							<a class="close" href="#">&times;</a>
						<div class="content">
					@article{ubaru2012gray,<br />
  					title={Gray scales by cross pairing of select and data voltages in multi-line addressed liquid crystal displays}, <br />
  					author={Ubaru, Shashanka and Ruckmongathan, Temkar N},
  					journal={Journal of Display Technology},<br />
  					volume={8},<br />
  					number={11},<br />
  					pages={669--677},<br />
  					year={2012},<br />
  					publisher={IEEE}}
 			                </div>
						</div>
					</div>		
				</ol>	

				
									
				<h3>Thesis </h3>
				<h5> Masters Thesis</h5>
				<ol reversed>
				<li><i>"Randomized techniques for matrix decomposition and estimating the approximate rank of a matrix"</i>, M. S. Thesis, University of Minnesota, Minneapolis, Nov. 2014.</li>
				[<a href="http://conservancy.umn.edu/handle/11299/170049" target="_blank"><i class="fa fa-external-link"></i> Link</a>, <a href="http://conservancy.umn.edu/bitstream/handle/11299/170049/Ubaru_umn_0130M_15586.pdf?sequence=1&isAllowed=y" target="_blank"><i class="fa fa-file-pdf-o"></i> PDF</a>]
				</ol>

			

		</section>
				<!-- Five -->			
					<section id="five">
<span style="float:right;">
				
					&copy; Shashanka Ubaru 2016 | Design: <a href="http://html5up.net">HTML5 UP</a>
				
</span>
			</section>
</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
